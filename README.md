# Human vs. AI Text Classifier (MLOps Project)

This project implements a robust text classification system to determine whether a given text is written by a human or generated by an AI model. It follows modern MLOps principles for modularity, reproducibility, experimentation, retraining, and deployment.

The system compares multiple model architectures (LSTM, BERT, RoBERTa), tracks experiments with MLflow, manages configurations with Hydra, and versions data/models with DVC. A FastAPI provides endpoints for prediction and retraining.

## Features

*   **Text Classification**: Binary classification (Human vs. AI-generated text).
*   **Multiple Models**: Implements and compares LSTM, BERT, and RoBERTa architectures.
*   **MLOps Pipeline**: Full pipeline from data ingestion to model evaluation.
    *   Data Ingestion & Preprocessing
    *   Data Validation
    *   Model Training & Experiment Tracking (MLflow)
    *   Model Evaluation & Comparison
*   **Configuration Management**: Uses Hydra for flexible and overridable configurations.
*   **Experiment Tracking**: Leverages MLflow for logging metrics, parameters, and model versions.
*   **Data & Model Versioning**: Uses DVC to track datasets and model artifacts.
*   **API Endpoints**: FastAPI application for:
    *   `POST /predict`: Get classification for input text.
    *   `POST /retrain`: Trigger model retraining (simulated for background task).
    *   `GET /stats`: View basic model performance statistics.
*   **Containerization**: Dockerfile for building a portable application image.
*   **Local Orchestration**: Docker Compose for running the app and MLflow server.
*   **CI/CD**: GitHub Actions for continuous integration (linting, testing, Docker build) and pipeline execution.

## Project Structure


.
├── .dvc/                   # DVC metadata
├── .github/workflows/      # GitHub Actions CI/CD workflows
├── api/                    # FastAPI application code
├── artifacts/              # DVC-tracked outputs (data, models, reports) - Gitignored
├── config/                 # Hydra configuration files (YAML)
├── logs/                   # Log files
├── mlruns/                 # MLflow experiment tracking data
├── src/                    # Main Python package for the text classifier
│   └── text_classifier/
│       ├── __init__.py
│       ├── components/     # Pipeline components (ingestion, processing, training, etc.)
│       ├── config/         # Configuration loading logic
│       ├── constants/      # Project-wide constants
│       ├── entity/         # Data entities/dataclasses
│       ├── models/         # Model architecture definitions (PyTorch Lightning)
│       ├── pipeline/       # Pipeline stage definitions
│       └── utils/          # Utility scripts
├── tests/                  # Pytest tests (to be added)
├── .dockerignore
├── .dvcignore
├── .gitignore
├── Dockerfile              # For building the application container
├── docker-compose.yaml     # For local multi-container orchestration
├── entrypoint.sh           # Entrypoint script for Docker container
├── main.py                 # Script to run the MLOps pipeline stages
├── README.md
└── requirements.txt        # Python dependencies


## Technical Stack & Tools

*   **Frameworks**: PyTorch, PyTorch Lightning
*   **NLP Transformers**: Hugging Face Transformers (for BERT, RoBERTa)
*   **Experiment Tracking**: MLflow
*   **Configuration Management**: Hydra
*   **Data Version Control**: DVC
*   **API**: FastAPI, Uvicorn
*   **Containerization**: Docker, Docker Compose
*   **CI/CD**: GitHub Actions
*   **Orchestration (Conceptual)**: Designed for future deployment with tools like Kubernetes.

## Setup and Installation

### Prerequisites

*   Python 3.9+
*   Docker & Docker Compose
*   Git
*   DVC (`pip install dvc dvc-s3` or your specific remote)

### 1. Clone the Repository

```bash
git clone https://github.com/mohamed-stifi/robust-ai-text-detector-mlops.git
cd robust-ai-text-detector-mlops
```

### 2. Set up Python Environment (Recommended: Virtual Environment)

```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
pip install -r requirements.txt
```

### 3. Initialize DVC (if not already cloned with DVC setup)

If this is a fresh setup where DVC remotes need configuration:
```bash
dvc init
# Example: Configure DVC remote (replace with your actual remote)
# dvc remote add -d myremote s3://your-s3-bucket/dvc-store/text-classifier
# Or for local testing:
# mkdir -p ../dvc_remote_storage
# dvc remote add -d myremote ../dvc_remote_storage
```

### 4. Pull DVC Data

This will download the versioned datasets and model artifacts.
```bash
dvc pull -R
```
*(Note: For the initial run, you might need to run the data ingestion pipeline first if no data is pre-committed to DVC remote).*

### 5. Dataset

The primary dataset used is "AI Vs Human Text" from Kaggle: [Link to Dataset](https://www.kaggle.com/datasets/shanakyasingh/datasets-for-human-and-ai-generated-text).
You will need to download it manually (e.g., `AI_Vs_Human_Full.csv.zip`) and place it in `artifacts/data_ingestion/data.zip` if the `source_URL` in `config/config.yaml` is not a direct download link or if you prefer manual download. The data ingestion pipeline will then process it.

## Running the MLOps Pipeline

You can run the entire pipeline or individual stages using `main.py`.

### Run the Full Pipeline (Local Python Environment)

```bash
python main.py --stage all
```

### Run Specific Stages

```bash
python main.py --stage ingestion
python main.py --stage validation
python main.py --stage transformation
python main.py --stage training
python main.py --stage evaluation
```

### Run with Docker (via entrypoint.sh)

Ensure the Docker image is built (`docker-compose build app` or `docker build -t human-ai-classifier .`).

```bash
# Run the full pipeline inside Docker
docker run --rm -v $(pwd)/artifacts:/app/artifacts -v $(pwd)/mlruns:/app/mlruns -v $(pwd)/config:/app/config human-ai-classifier pipeline all

# Run a specific stage inside Docker
docker run --rm -v $(pwd)/artifacts:/app/artifacts -v $(pwd)/mlruns:/app/mlruns -v $(pwd)/config:/app/config human-ai-classifier pipeline training
```

### Run with Docker Compose (Recommended for local development)

```bash
# Run the full pipeline
docker-compose run --rm app pipeline all

# Run a specific stage
docker-compose run --rm app pipeline training
```

## Running the API

### Using Docker Compose (Recommended)

This also starts the MLflow server.
```bash
docker-compose up --build app mlflow_server # Add --build only for the first time or when code changes
```
The API will be accessible at `http://localhost:8000`.
The MLflow UI will be accessible at `http://localhost:5000`.

### Locally (Python Environment)

Ensure `PYTHONPATH` includes the `src` directory or install the package.
```bash
export PYTHONPATH=$(pwd)/src:$(pwd) # Or set it appropriately
python api/main.py
```
The API will be accessible at `http://localhost:8000`.

### API Endpoints

*   `POST /predict`:
    *   Payload: `{"text": "Your text to classify here...", "model_type": "bert"}` (model_type is optional, defaults to best)
    *   Response: `{"prediction": "Human", "label": 0, "confidence_score": 0.95, "model_used": "bert"}`
*   `POST /retrain`:
    *   Payload: (None)
    *   Response: `{"message": "Retraining process initiated...", "status": "initiated"}`
*   `GET /stats`:
    *   Response: Model performance statistics.

API documentation is available at `http://localhost:8000/docs` when the API is running.

## Experiment Tracking with MLflow

Experiments, parameters, metrics, and model artifacts are logged to MLflow.
If using Docker Compose, the MLflow UI is available at `http://localhost:5000`.
The MLflow tracking URI is configured in `config/config.yaml` and can be overridden by environment variables (e.g., `MLFLOW_TRACKING_URI`).

## Data and Model Versioning with DVC

Large data files and model artifacts are versioned using DVC and stored in a configured remote (e.g., S3, local directory).
The `.dvc` files (pointers) are committed to Git.

### Typical DVC Workflow:

1.  **Modify data/models** (e.g., by running the pipeline).
2.  **Track changes**: `dvc add artifacts/data_transformation`
3.  **Commit pointers**: `git add artifacts/data_transformation.dvc && git commit -m "Update transformed data"`
4.  **Push data to remote**: `dvc push`
5.  **Push Git commits**: `git push`
6.  **To get data on another machine/clone**: `git pull && dvc pull`

## CI/CD with GitHub Actions

*   **`ci.yaml`**: Triggered on pushes/PRs to `main`/`develop` and tags.
    *   Lints and formats code (Flake8, Black).
    *   (Future) Runs Pytests.
    *   Builds and pushes a Docker image to a container registry (e.g., GHCR) on pushes to `main` or new tags.
*   **`run_pipeline.yaml`**: Triggered manually via `workflow_dispatch` or on a schedule.
    *   Checks out code.
    *   Pulls DVC data.
    *   Runs the specified MLOps pipeline stage(s).
    *   Adds and pushes new/changed DVC artifacts.
    *   Commits and pushes updated `.dvc` pointer files to Git.

## Future Work & Improvements

*   Implement comprehensive unit and integration tests.
*   Advanced model monitoring for data drift and concept drift.
*   More robust secrets management for production (e.g., HashiCorp Vault).
*   Scalable API deployment (e.g., Kubernetes, AWS ECS/EKS).
*   Hyperparameter optimization as a separate pipeline step.
*   User interface for easier interaction.
*   Full implementation of background retraining tasks using a message queue (e.g., Celery, RabbitMQ).

## Contributing

Contributions are welcome! Please follow standard Git practices (fork, feature branch, pull request). Ensure code is linted and (eventually) tests pass.

