# AI Text Generation Detection - MLOps End-to-End Project

This repository contains the source code and documentation for an MLOps-driven system designed to detect whether a given text is written by a human or generated by an AI. The project encompasses the entire machine learning lifecycle, from data ingestion and preprocessing to model training, hyperparameter optimization, evaluation, and deployment via a web application with an API backend.

## Table of Contents

1.  [Introduction](#introduction)
2.  [Features](#features)
3.  [System Architecture](#system-architecture)
    *   [Backend (MLOps Pipeline & API)](#backend-mlops-pipeline--api)
    *   [Frontend (User Interface)](#frontend-user-interface)
    *   [Asynchronous Task Processing (Kafka)](#asynchronous-task-processing-kafka)
4.  [MLOps Pipeline Stages](#mlops-pipeline-stages)
5.  [Technologies Used](#technologies-used)
6.  [Project Structure](#project-structure)
7.  [Setup and Installation](#setup-and-installation)
    *   [Prerequisites](#prerequisites)
    *   [Local Setup (Python MLOps Backend)](#local-setup-python-mlops-backend)
    *   [Local Setup (Next.js Frontend)](#local-setup-nextjs-frontend)
    *   [Dockerized Setup (Recommended)](#dockerized-setup-recommended)
8.  [Running the System](#running-the-system)
    *   [Running MLOps Pipelines](#running-mlops-pipelines)
    *   [Using the Web Application](#using-the-web-application)
9.  [Experiment Tracking (MLflow)](#experiment-tracking-mlflow)
10. [Hyperparameter Optimization](#hyperparameter-optimization)
11. [Results Highlights](#results-highlights)
12. [Limitations](#limitations)
13. [Future Work](#future-work)
14. [Contributing](#contributing)

## Introduction

The proliferation of advanced AI language models presents a challenge in distinguishing AI-generated text from human-written content. This project implements an end-to-end MLOps system to address this, providing a robust, reproducible, and maintainable solution for training, evaluating, and deploying text classification models. The system emphasizes automation, continuous monitoring principles, and operational best practices.

## Features

*   **End-to-End ML Pipeline:** Covers data ingestion, validation, transformation, HPO, parameter updates, model training, and evaluation.
*   **Multiple Model Support:** Trains and evaluates LSTM, BERT, and RoBERTa models.
*   **Hyperparameter Optimization:** Integrated Optuna for automated tuning of model hyperparameters.
*   **Experiment Tracking:** Uses MLflow to log all experiments, parameters, metrics, and artifacts.
*   **Configuration Driven:** Utilizes YAML files for managing all pipeline and model configurations.
*   **Asynchronous Task Processing:** Employs Kafka for handling long-running MLOps tasks (evaluation, HPO, retraining) without blocking the API.
*   **REST API:** FastAPI backend for model predictions, serving statistics, and triggering MLOps tasks.
*   **Interactive Frontend:** Next.js application for user interaction, text submission, results visualization, and admin controls.
*   **Containerized Deployment:** Docker and Docker Compose for consistent environments and easy local deployment of all services.
*   **Modular Design:** Clear separation of concerns between pipeline stages, components, and services.


### Backend (MLOps Pipeline & API)

*   **Language:** Python
*   **Core Libraries:** PyTorch, PyTorch Lightning, Hugging Face Transformers, Scikit-learn, Pandas, Optuna.
*   **Pipelines:** A series of scriptable stages (`src/text_classifier/pipeline/`) managing the ML lifecycle.
*   **Configuration:** Managed by `ConfigurationManager` loading `config/config.yaml` and `config/params.yaml`.
*   **API Server:** FastAPI (`api/main.py`) serves predictions and acts as a Kafka producer for MLOps tasks.
*   **Experiment Tracking:** MLflow server for logging all runs and artifacts.

### Frontend (User Interface)

*   **Framework:** Next.js (React)
*   **Key Libraries:** Zustand (state management), React Query (server state), Tailwind CSS, shadcn/ui (components), Recharts (plotting).
*   **Functionality:**
    *   `/predict`: Submit text for classification.
    *   `/stats`: View model performance, including charts.
    *   `/admin`: Trigger MLOps tasks (retrain, evaluate, HPO) and view system status.


### Asynchronous Task Processing (Kafka)

Long-running MLOps tasks are handled asynchronously:
1.  Frontend requests a task (e.g., HPO) via the Admin page.
2.  FastAPI backend receives the request and publishes a message to a Kafka topic (e.g., `mlops_tasks`).
3.  A dedicated Python Kafka Consumer service (`src/text_classifier/kafka_consumer_service.py`) consumes the message and executes the corresponding MLOps pipeline stage.

## MLOps Pipeline Stages

The core MLOps backend is structured into the following automated stages:

1.  **Data Ingestion:** Downloads, extracts, and samples the dataset.
2.  **Data Validation:** Verifies the integrity and presence of required data files.
3.  **Data Transformation:** Preprocesses text (cleaning, stopword removal, balancing), splits into train/val/test, and tokenizes.
4.  **Hyperparameter Optimization (HPO):** Uses Optuna to find optimal hyperparameters for each model type, logging results to MLflow.
5.  **Parameter Update:** Reads HPO summary and updates `params.yaml` with the best hyperparameters.
6.  **Model Training:** Trains LSTM, BERT, and RoBERTa models using (HPO-updated) parameters, logging to MLflow.
7.  **Model Evaluation:** Evaluates trained models, generates reports and plots, and logs results to MLflow.

## Technologies Used

| Category          | Technology / Library                                     |
|-------------------|----------------------------------------------------------|
| **Backend & ML**  | Python, PyTorch, PyTorch Lightning, Transformers, Scikit-learn, Pandas, NLTK, FuzzyWuzzy, Optuna, Joblib |
| **API**           | FastAPI, Uvicorn                                         |
| **Message Queue** | Apache Kafka, kafka-python                               |
| **Experiment Tracking** | MLflow                                               |
| **Frontend**      | Next.js, React, TypeScript, Zustand, React Query, Axios  |
| **UI / Styling**  | Tailwind CSS, shadcn/ui, Recharts, Lucide Icons          |
| **Containerization**| Docker, Docker Compose                                   |
| **Version Control**| Git, GitHub                                              |
| **Configuration** | YAML (pyyaml, python-box)                                |

## Project Structure



```
pfa-s4/  (Main Project Root)
├── api/                          # FastAPI application code
├── config/                       # Configuration files (config.yaml, params.yaml)
├── mlops-frontend/               # Next.js frontend project
│   ├── app/                      # Frontend pages and layout (App Router)
│   ├── components/               # React components
│   ├── lib/                      # Frontend utilities, API client
│   ├── store/                    # Zustand state stores
│   ├── hooks/                    # React Query custom hooks
│   ├── public/                   # Static assets for frontend
│   ├── Dockerfile                # Dockerfile for frontend
│   └── ...                       # Other frontend config files (tailwind, next.js)
├── src/
│   └── text_classifier/          # Core Python MLOps package
│       ├── components/           # Pipeline stage components
│       ├── config/               # Configuration manager
│       ├── constants/            # Project constants
│       ├── entity/               # Configuration entities (dataclasses)
│       ├── models/               # PyTorch Lightning model definitions
│       ├── pipeline/             # MLOps pipeline stage definitions
│       ├── utils/                # Common utilities, Kafka producer
│       ├── __init__.py           # Package initializer, logger setup
│       └── kafka_consumer_service.py # Kafka consumer for async tasks
├── artifacts/                    # Stores data, models, reports (Gitignored - use DVC for large files)
├── logs/                         # Application logs
├── mlruns/                       # MLflow experiment tracking data (Gitignored)
├── .dockerignore                 # Ignores files for backend Docker build context
├── .gitignore                    # Specifies intentionally untracked files for Git
├── Dockerfile                    # Dockerfile for backend/consumer services
├── docker-compose.yaml           # Docker Compose file for orchestrating all services
├── entrypoint.sh                 # Entrypoint script for backend Docker containers
├── main.py                       # Main script to run MLOps pipelines
├── README.md                     # This file
├── requirements.txt              # Python dependencies
├── setup.py                      # Python package setup
└── test_mlops_pipelines.py       # Notebook/script for testing pipelines
```


## Setup and Installation

### Prerequisites

*   Git
*   Python (3.9+ recommended)
*   Node.js (18.x or later recommended) & npm (or yarn/pnpm)
*   Docker & Docker Compose
*   (Optional) Kaggle API token configured for data download (see `src/text_classifier/components/data_ingestion.py` comments if needed).

### Local Setup (Python MLOps Backend)

For developing or running the Python backend components individually (without full Docker orchestration):

1.  **Clone the repository:**
    ```bash
    git clone https://github.com/mohamed-stifi/End-to-End-MLOps-Pipeline-for-AI-Text-Generation-Detection.git
    cd End-to-End-MLOps-Pipeline-for-AI-Text-Generation-Detection
    ```
2.  **Create and activate a virtual environment:**
    ```bash
    python -m venv pfa-venv
    source pfa-venv/bin/activate  # On Windows: pfa-venv\Scripts\activate
    ```
3.  **Install Python dependencies:**
    ```bash
    pip install -r requirements.txt
    ```
    This also installs the `text_classifier` package in editable mode due to `-e .` in `requirements.txt`.

### Local Setup (Next.js Frontend)

For developing the frontend separately:

1.  **Navigate to the frontend directory:**
    ```bash
    cd mlops-frontend
    ```
2.  **Install Node.js dependencies:**
    ```bash
    npm install
    ```
3.  **Create `.env.local` file:**
    In `mlops-frontend/`, create `.env.local` with:
    ```env
    NEXT_PUBLIC_API_BASE_URL=http://localhost:8000 # Or your backend API URL
    ```
4.  **Run the frontend development server:**
    ```bash
    npm run dev
    ```
    The frontend will be accessible at `http://localhost:3000`. (Ensure the backend API is running).

### Dockerized Setup (Recommended)

This is the recommended way to run the entire system with all its interconnected services.

1.  **Clone the repository (if not already done):**
    ```bash
    git clone https://github.com/mohamed-stifi/End-to-End-MLOps-Pipeline-for-AI-Text-Generation-Detection.git
    cd End-to-End-MLOps-Pipeline-for-AI-Text-Generation-Detection
    ```
2.  **Ensure `mlops-frontend/next.config.js` has `output: 'standalone'`:**
    ```javascript
    // mlops-frontend/next.config.js
    /** @type {import('next').NextConfig} */
    const nextConfig = {
      reactStrictMode: true,
      output: 'standalone', // Ensure this line is present
    };
    module.exports = nextConfig;
    ```
3.  **Build and start all services using Docker Compose:**
    From the project root (`work_dir/`):
    ```bash
    docker-compose up --build -d
    ```
    *   `--build`: Rebuilds images if Dockerfiles or their contexts have changed.
    *   `-d`: Runs services in detached (background) mode.

    Wait for all services to start. You can check logs using `docker-compose logs -f <service_name>`.

## Running the System

### Running MLOps Pipelines

A demonstration of executing these pipelines individually can be found in the Jupyter Notebook: `notebooks/test_mlops_pipelines.ipynb`. If this notebook doesn't render directly on GitHub, please download it and open it using VS Code or Jupyter Notebook/Lab to see the execution flow and outputs of each stage.

Once the Python environment is set up (either locally or via Docker as described in the [Setup and Installation](#setup-and-installation) section), you can run the MLOps pipeline stages directly from your terminal.

*   **Run all stages sequentially:**
    From the project root (`pfa-s4/`):
    ```bash
    python main.py --stage all
    ```
    If you are executing this command inside one of the backend Docker containers (e.g., `app` or `mlops_consumer`), the path would typically be:
    ```bash
    python /app/main.py --stage all
    ```
    (This assumes the `WORKDIR` in your backend `Dockerfile` is `/app` and `main.py` is in that directory within the container.)

*   **Run a specific stage:**
    From the project root (`pfa-s4/`):
    ```bash
    python main.py --stage ingestion
    python main.py --stage hpo
    # ... and so on for other stages
    ```
    Available stages that can be run individually are: `ingestion`, `validation`, `transformation`, `hpo`, `update_params`, `training`, `evaluation`.

*   **Triggering Pipelines Via Frontend Admin UI (Dockerized Setup):**
    With the full system running via Docker Compose, navigate to `http://localhost:3000/admin` in your web browser. On this page, you will find buttons to trigger:
    *   "Full Retraining"
    *   "Evaluate Models"
    *   "Tune Hyperparameters"

    When these buttons are clicked, a request is sent to the backend API, which then publishes a task message to Kafka. The `mlops_consumer` service picks up these tasks and executes the corresponding pipelines asynchronously.

    You can monitor the logs of the consumer to see these tasks being processed:
    ```bash
    docker-compose logs -f mlops_consumer
    ```
    **Note:** Due to local machine resource limitations (as detailed in the [Limitations](#limitations) section), thoroughly testing the end-to-end execution of these long-running tasks (especially "Full Retraining" and "Tune Hyperparameters") triggered via the frontend and processed by the Kafka consumer in the Dockerized environment was constrained. The individual pipeline stages have been tested using the `test_mlops_pipelines.ipynb` notebook.

### Using the Web Application (Dockerized Setup)

*   **Frontend Application:** `http://localhost:3000`
    *   Navigate to `/predict` for text classification.
    *   Navigate to `/stats` to view model performance.
    *   Navigate to `/admin` to trigger MLOps tasks.
*   **Backend API Documentation (Swagger UI):** `http://localhost:8000/docs`
*   **MLflow UI:** `http://localhost:5000`
*   **Kafka Broker (for external tools, if needed):** `localhost:9092`

### Using the Web Application (Dockerized Setup)

*   **Frontend Application:** `http://localhost:3000`
*   **Backend API Docs:** `http://localhost:8000/docs`
*   **MLflow UI:** `http://localhost:5000`
*   **Kafka (if you have management tools like Kowl):** Connect to `localhost:9092`.

## Experiment Tracking (MLflow)

*   All HPO trials, model training runs, and evaluations are logged to MLflow.
*   Access the MLflow UI at `http://localhost:5000` (when running via Docker Compose).
*   Experiments are named e.g., `text_classification_lstm`, `text_classifier_hpo_bert_data_transformation`, `model_evaluation`.
*   You can view parameters, metrics, artifacts (model files, plots, reports) for each run.

## Hyperparameter Optimization

*   The HPO pipeline (`stage_06_...`) uses Optuna to find the best parameters.
*   Results are saved to `artifacts/hyperparameter_optimization/hpo_optimization_summary.json`.
*   The `stage_07_update_parameters` pipeline updates `config/params.yaml` with these optimal values before the main model training.

## Limitations

*   **Local Resource Constraints:** Training and HPO for large datasets/complex models can be slow on the specified local hardware. Google Colab was used as a workaround for some pipeline tests.
*   **Dataset Size:** The `max_sample_number` for development limits the data used, potentially affecting model generalization.
*   **Basic Kafka Feedback:** The current Kafka implementation is fire-and-forget. The frontend doesn't get real-time status updates on long-running tasks beyond initial submission confirmation.
*   **Limited CI/CD:** Full CI/CD automation (e.g., GitHub Actions) is conceptual and not yet implemented.
*   **DVC Not Yet Integrated:** Versioning of large data and model artifacts with DVC is planned for future work.
*   **Error Handling in Consumer:** Kafka consumer error handling is basic (logging); a dead-letter queue or more robust retry logic could be added.
*   **Security:** Kafka and other services are run without advanced security configurations, suitable for local development only.

## Future Work

*   **Cloud Deployment:** Migrate services (Kafka, MLflow, API, Consumer, Frontend) to a cloud platform (AWS, GCP, Azure) for scalability and production readiness.
*   **DVC Integration:** Implement DVC for versioning datasets, intermediate artifacts, and large model files.
*   **Full CI/CD Pipelines:** Set up GitHub Actions for automated testing, building Docker images, and deployment.
*   **Advanced Monitoring:** Integrate tools like Prometheus/Grafana for system monitoring and Evidently AI/NannyML for model/data drift detection.
*   **Real-time Task Status:** Implement a mechanism (e.g., WebSockets, or consumer writing status to a DB/Kafka topic polled by frontend) for real-time updates on MLOps tasks.
*   **Model Registry Enhancements:** Utilize MLflow Model Registry more formally for staging and promoting models to production.
*   **Scalability Improvements:** Explore scaling Kafka consumers and API instances.
*   **Enhanced Model Fine-tuning:** Experiment with unfreezing more layers of transformer models and using larger datasets.
*   **Security Hardening:** Implement proper authentication, authorization, and network security for all services in a production setting.

## Contributing

Contributions are welcome! Please follow these steps:
1.  Fork the repository.
2.  Create a new branch (`git checkout -b feature/YourFeatureName`).
3.  Make your changes.
4.  Commit your changes (`git commit -m 'Add some YourFeatureName'`).
5.  Push to the branch (`git push origin feature/YourFeatureName`).
6.  Open a Pull Request.

Please ensure your code adheres to existing style guidelines and all tests pass.

---