{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c92bcac8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/mohamed-stifi/Desktop/pfa-s4/notebooks'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/mohamed-stifi/Desktop/pfa-s4'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('..')\n",
    "\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/mohamed-stifi/Desktop/pfa-s4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mohamed-stifi/Desktop/pfa-s4/pfa-venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from text_classifier.components.model_trainer import TextDataset, ModelTrainer\n",
    "from text_classifier.config.configuration import ConfigurationManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-29 18:39:24,840: INFO: common: text_classifier.utils.common: yaml file: /home/mohamed-stifi/Desktop/pfa-s4/config/config.yaml loaded successfully]\n",
      "[2025-05-29 18:39:24,908: INFO: common: text_classifier.utils.common: yaml file: /home/mohamed-stifi/Desktop/pfa-s4/config/params.yaml loaded successfully]\n",
      "[2025-05-29 18:39:24,912: INFO: common: text_classifier.utils.common: created directory at: artifacts]\n",
      "[2025-05-29 18:39:24,914: INFO: common: text_classifier.utils.common: created directory at: artifacts/model_trainer]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ModelTrainerConfig(root_dir='artifacts/model_trainer', data_path='artifacts/data_transformation', model_name='bert-base-uncased', num_train_epochs=5, warmup_ratio=0.1, per_device_train_batch_size=16, per_device_eval_batch_size=16, weight_decay=0.01, logging_steps=10, evaluation_strategy='epoch', eval_steps=500, save_steps=500, gradient_accumulation_steps=1, learning_rate=2e-05)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = ConfigurationManager()\n",
    "\n",
    "model_trainer_config = config.get_model_trainer_config()\n",
    "model_trainer_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "882e0e77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModelTrainerConfig(root_dir='artifacts/model_trainer', data_path='artifacts/data_transformation', model_name='bert-base-uncased', num_train_epochs=5, warmup_ratio=0.1, per_device_train_batch_size=8, per_device_eval_batch_size=8, weight_decay=0.01, logging_steps=10, evaluation_strategy='epoch', eval_steps=5, save_steps=5, gradient_accumulation_steps=1, learning_rate=2e-05)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_trainer_config.eval_steps = 5\n",
    "model_trainer_config.save_steps = 5\n",
    "model_trainer_config.per_device_eval_batch_size = 8\n",
    "model_trainer_config.per_device_train_batch_size = 8\n",
    "\n",
    "model_trainer_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e0f57c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<text_classifier.components.model_trainer.ModelTrainer at 0x7318204829c0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_trainer = ModelTrainer(model_trainer_config)\n",
    "\n",
    "model_trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684d3886",
   "metadata": {},
   "source": [
    "## Prepare data loaders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a9fd46",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_train_loader, lstm_val_loader, lstm_test_loader, lstm_vocab_size = model_trainer.prepare_data_loaders('lstm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'attention_mask', 'labels'])\n",
      "input_ids:  torch.Size([8, 512])\n",
      "attention_mask:  torch.Size([8, 512])\n",
      "labels:  torch.Size([8])\n"
     ]
    }
   ],
   "source": [
    "for batch in lstm_train_loader:\n",
    "    print(batch.keys())\n",
    "    print('input_ids: ', batch['input_ids'].shape)\n",
    "    print('attention_mask: ', batch['attention_mask'].shape)\n",
    "    print('labels: ', batch['labels'].shape)\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "82558d84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3197"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTMClassifier(\n",
       "  (embedding): Embedding(3197, 128, padding_idx=0)\n",
       "  (lstm): LSTM(128, 256, num_layers=2, batch_first=True, dropout=0.3, bidirectional=True)\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       "  (classifier): Linear(in_features=512, out_features=2, bias=True)\n",
       "  (criterion): CrossEntropyLoss()\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm = model_trainer.get_model('lstm', lstm_vocab_size)\n",
    "lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ba02e22b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-28 23:51:20,839: INFO: model_trainer: textClassifierLogger: Starting training for lstm]\n",
      "[2025-05-28 23:51:21,395: INFO: setup: pytorch_lightning.utilities.rank_zero: GPU available: False, used: False]\n",
      "[2025-05-28 23:51:21,400: INFO: setup: pytorch_lightning.utilities.rank_zero: TPU available: False, using: 0 TPU cores]\n",
      "[2025-05-28 23:51:21,402: INFO: setup: pytorch_lightning.utilities.rank_zero: HPU available: False, using: 0 HPUs]\n",
      "[2025-05-28 23:51:21,435: INFO: model_summary: pytorch_lightning.callbacks.model_summary: \n",
      "  | Name       | Type             | Params | Mode \n",
      "--------------------------------------------------------\n",
      "0 | embedding  | Embedding        | 409 K  | train\n",
      "1 | lstm       | LSTM             | 2.4 M  | train\n",
      "2 | dropout    | Dropout          | 0      | train\n",
      "3 | classifier | Linear           | 1.0 K  | train\n",
      "4 | criterion  | CrossEntropyLoss | 0      | train\n",
      "--------------------------------------------------------\n",
      "2.8 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.8 M     Total params\n",
      "11.111    Total estimated model params size (MB)\n",
      "5         Modules in train mode\n",
      "0         Modules in eval mode]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mohamed-stifi/Desktop/pfa-s4/pfa-venv/lib/python3.12/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/mohamed-stifi/Desktop/pfa-s4/artifacts/model_trainer/lstm exists and is not empty.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mohamed-stifi/Desktop/pfa-s4/pfa-venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mohamed-stifi/Desktop/pfa-s4/pfa-venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  28%|██▊       | 5/18 [02:23<06:12,  0.03it/s, v_num=ae42, train_loss=0.694, train_acc=0.500, val_loss=0.691, val_acc=0.600][2025-05-28 23:53:46,591: INFO: model_checkpoint: pytorch_lightning.utilities.rank_zero: Epoch 0, global step 5: 'val_acc' reached 0.60000 (best 0.60000), saving model to '/home/mohamed-stifi/Desktop/pfa-s4/artifacts/model_trainer/lstm/lstm-epoch=00-val_acc=0.60.ckpt' as top 1]\n",
      "Epoch 0:  56%|█████▌    | 10/18 [04:45<03:48,  0.03it/s, v_num=ae42, train_loss=0.688, train_acc=0.625, val_loss=0.690, val_acc=0.650][2025-05-28 23:56:09,253: INFO: model_checkpoint: pytorch_lightning.utilities.rank_zero: Epoch 0, global step 10: 'val_acc' reached 0.65000 (best 0.65000), saving model to '/home/mohamed-stifi/Desktop/pfa-s4/artifacts/model_trainer/lstm/lstm-epoch=00-val_acc=0.65.ckpt' as top 1]\n",
      "Epoch 0:  83%|████████▎ | 15/18 [07:30<01:30,  0.03it/s, v_num=ae42, train_loss=0.697, train_acc=0.375, val_loss=0.689, val_acc=0.650][2025-05-28 23:58:53,896: INFO: model_checkpoint: pytorch_lightning.utilities.rank_zero: Epoch 0, global step 15: 'val_acc' was not in top 1]\n",
      "Epoch 1:  28%|██▊       | 5/18 [02:39<06:54,  0.03it/s, v_num=ae42, train_loss=0.692, train_acc=0.500, val_loss=0.687, val_acc=0.650] [2025-05-29 00:02:56,895: INFO: model_checkpoint: pytorch_lightning.utilities.rank_zero: Epoch 1, global step 23: 'val_acc' was not in top 1]\n",
      "Epoch 1:  56%|█████▌    | 10/18 [05:37<04:30,  0.03it/s, v_num=ae42, train_loss=0.678, train_acc=0.875, val_loss=0.686, val_acc=0.700][2025-05-29 00:05:55,319: INFO: model_checkpoint: pytorch_lightning.utilities.rank_zero: Epoch 1, global step 28: 'val_acc' reached 0.70000 (best 0.70000), saving model to '/home/mohamed-stifi/Desktop/pfa-s4/artifacts/model_trainer/lstm/lstm-epoch=01-val_acc=0.70.ckpt' as top 1]\n",
      "Epoch 1:  83%|████████▎ | 15/18 [08:17<01:39,  0.03it/s, v_num=ae42, train_loss=0.681, train_acc=0.750, val_loss=0.685, val_acc=0.700][2025-05-29 00:08:34,578: INFO: model_checkpoint: pytorch_lightning.utilities.rank_zero: Epoch 1, global step 33: 'val_acc' was not in top 1]\n",
      "Epoch 2:  28%|██▊       | 5/18 [02:52<07:29,  0.03it/s, v_num=ae42, train_loss=0.691, train_acc=0.500, val_loss=0.683, val_acc=0.800] [2025-05-29 00:12:55,115: INFO: model_checkpoint: pytorch_lightning.utilities.rank_zero: Epoch 2, global step 41: 'val_acc' reached 0.80000 (best 0.80000), saving model to '/home/mohamed-stifi/Desktop/pfa-s4/artifacts/model_trainer/lstm/lstm-epoch=02-val_acc=0.80.ckpt' as top 1]\n",
      "Epoch 2:  56%|█████▌    | 10/18 [05:13<04:10,  0.03it/s, v_num=ae42, train_loss=0.672, train_acc=0.875, val_loss=0.682, val_acc=0.800][2025-05-29 00:15:15,468: INFO: model_checkpoint: pytorch_lightning.utilities.rank_zero: Epoch 2, global step 46: 'val_acc' was not in top 1]\n",
      "Epoch 2:  83%|████████▎ | 15/18 [07:40<01:32,  0.03it/s, v_num=ae42, train_loss=0.676, train_acc=1.000, val_loss=0.681, val_acc=0.800][2025-05-29 00:17:43,103: INFO: model_checkpoint: pytorch_lightning.utilities.rank_zero: Epoch 2, global step 51: 'val_acc' was not in top 1]\n",
      "Epoch 3:  28%|██▊       | 5/18 [02:40<06:57,  0.03it/s, v_num=ae42, train_loss=0.668, train_acc=0.750, val_loss=0.679, val_acc=0.800] [2025-05-29 00:21:44,300: INFO: model_checkpoint: pytorch_lightning.utilities.rank_zero: Epoch 3, global step 59: 'val_acc' was not in top 1]\n",
      "Epoch 3:  28%|██▊       | 5/18 [02:40<06:57,  0.03it/s, v_num=ae42, train_loss=0.668, train_acc=0.750, val_loss=0.679, val_acc=0.800]\n",
      "[2025-05-29 00:21:44,347: INFO: checkpoint_connector: pytorch_lightning.utilities.rank_zero: Restoring states from the checkpoint path at /home/mohamed-stifi/Desktop/pfa-s4/artifacts/model_trainer/lstm/lstm-epoch=02-val_acc=0.80.ckpt]\n",
      "[2025-05-29 00:21:44,533: INFO: checkpoint_connector: pytorch_lightning.utilities.rank_zero: Loaded model weights from the checkpoint at /home/mohamed-stifi/Desktop/pfa-s4/artifacts/model_trainer/lstm/lstm-epoch=02-val_acc=0.80.ckpt]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mohamed-stifi/Desktop/pfa-s4/pfa-venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 5/5 [00:02<00:00,  1.85it/s]\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        test_acc                   0.75\n",
      "         test_f1            0.7395833134651184\n",
      "     test_precision         0.7976190447807312\n",
      "       test_recall                 0.75\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/05/29 00:22:25 WARNING mlflow.utils.requirements_utils: The following packages were not found in the public PyPI package index as of 2025-04-15; if these packages are not present in the public PyPI index, you must install them manually before loading your model: {'text-classifier'}\n",
      "\u001b[31m2025/05/29 00:22:25 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-29 00:22:25,886: INFO: model_trainer: textClassifierLogger: Training completed for lstm]\n",
      "[2025-05-29 00:22:25,888: INFO: model_trainer: textClassifierLogger: Test results: {'test_acc': 0.75, 'test_precision': 0.7976190447807312, 'test_recall': 0.75, 'test_f1': 0.7395833134651184}]\n"
     ]
    }
   ],
   "source": [
    "test_results, model_path = model_trainer.train_model('lstm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_trainer.config.num_train_epochs = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4a4c5f",
   "metadata": {},
   "source": [
    "### bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert = model_trainer.get_model('bert')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f7b0820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable: classifier.weight\n",
      "Trainable: classifier.bias\n"
     ]
    }
   ],
   "source": [
    "for name, param in bert.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f\"Trainable: {name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d44a9f24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1538"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.numel() for p in bert.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3e473e05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-29 02:09:30,676: INFO: model_trainer: textClassifierLogger: Starting training for bert]\n",
      "[2025-05-29 02:09:32,073: INFO: setup: pytorch_lightning.utilities.rank_zero: GPU available: False, used: False]\n",
      "[2025-05-29 02:09:32,076: INFO: setup: pytorch_lightning.utilities.rank_zero: TPU available: False, using: 0 TPU cores]\n",
      "[2025-05-29 02:09:32,078: INFO: setup: pytorch_lightning.utilities.rank_zero: HPU available: False, using: 0 HPUs]\n",
      "[2025-05-29 02:09:32,149: INFO: model_summary: pytorch_lightning.callbacks.model_summary: \n",
      "  | Name       | Type             | Params | Mode \n",
      "--------------------------------------------------------\n",
      "0 | bert       | BertModel        | 109 M  | eval \n",
      "1 | dropout    | Dropout          | 0      | train\n",
      "2 | classifier | Linear           | 1.5 K  | train\n",
      "3 | criterion  | CrossEntropyLoss | 0      | train\n",
      "--------------------------------------------------------\n",
      "1.5 K     Trainable params\n",
      "109 M     Non-trainable params\n",
      "109 M     Total params\n",
      "437.935   Total estimated model params size (MB)\n",
      "3         Modules in train mode\n",
      "228       Modules in eval mode]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mohamed-stifi/Desktop/pfa-s4/pfa-venv/lib/python3.12/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/mohamed-stifi/Desktop/pfa-s4/artifacts/model_trainer/bert exists and is not empty.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mohamed-stifi/Desktop/pfa-s4/pfa-venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mohamed-stifi/Desktop/pfa-s4/pfa-venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  28%|██▊       | 5/18 [01:33<04:02,  0.05it/s, v_num=ae87, train_loss=0.793, train_acc=0.250, val_loss=0.766, val_acc=0.050][2025-05-29 02:11:41,185: INFO: model_checkpoint: pytorch_lightning.utilities.rank_zero: Epoch 0, global step 5: 'val_acc' reached 0.05000 (best 0.05000), saving model to '/home/mohamed-stifi/Desktop/pfa-s4/artifacts/model_trainer/bert/bert-epoch=00-val_acc=0.05.ckpt' as top 1]\n",
      "Epoch 0:  56%|█████▌    | 10/18 [02:48<02:14,  0.06it/s, v_num=ae87, train_loss=0.734, train_acc=0.375, val_loss=0.763, val_acc=0.100][2025-05-29 02:12:56,171: INFO: model_checkpoint: pytorch_lightning.utilities.rank_zero: Epoch 0, global step 10: 'val_acc' reached 0.10000 (best 0.10000), saving model to '/home/mohamed-stifi/Desktop/pfa-s4/artifacts/model_trainer/bert/bert-epoch=00-val_acc=0.10.ckpt' as top 1]\n",
      "Epoch 0:  83%|████████▎ | 15/18 [05:16<01:03,  0.05it/s, v_num=ae87, train_loss=0.768, train_acc=0.250, val_loss=0.759, val_acc=0.050][2025-05-29 02:15:24,288: INFO: model_checkpoint: pytorch_lightning.utilities.rank_zero: Epoch 0, global step 15: 'val_acc' was not in top 1]\n",
      "Epoch 0: 100%|██████████| 18/18 [05:53<00:00,  0.05it/s, v_num=ae87, train_loss=0.749, train_acc=0.500, val_loss=0.759, val_acc=0.050][2025-05-29 02:16:01,953: INFO: fit_loop: pytorch_lightning.utilities.rank_zero: `Trainer.fit` stopped: `max_epochs=1` reached.]\n",
      "Epoch 0: 100%|██████████| 18/18 [05:53<00:00,  0.05it/s, v_num=ae87, train_loss=0.749, train_acc=0.500, val_loss=0.759, val_acc=0.050]\n",
      "[2025-05-29 02:16:01,999: INFO: checkpoint_connector: pytorch_lightning.utilities.rank_zero: Restoring states from the checkpoint path at /home/mohamed-stifi/Desktop/pfa-s4/artifacts/model_trainer/bert/bert-epoch=00-val_acc=0.10.ckpt]\n",
      "[2025-05-29 02:16:10,710: INFO: checkpoint_connector: pytorch_lightning.utilities.rank_zero: Loaded model weights from the checkpoint at /home/mohamed-stifi/Desktop/pfa-s4/artifacts/model_trainer/bert/bert-epoch=00-val_acc=0.10.ckpt]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mohamed-stifi/Desktop/pfa-s4/pfa-venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 5/5 [00:58<00:00,  0.09it/s]\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        test_acc                   0.125\n",
      "         test_f1            0.12445278465747833\n",
      "     test_precision         0.12406015396118164\n",
      "       test_recall                 0.125\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/05/29 02:17:58 WARNING mlflow.utils.requirements_utils: The following packages were not found in the public PyPI package index as of 2025-04-15; if these packages are not present in the public PyPI index, you must install them manually before loading your model: {'text-classifier'}\n",
      "\u001b[31m2025/05/29 02:17:59 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-29 02:18:02,183: INFO: model_trainer: textClassifierLogger: Training completed for bert]\n",
      "[2025-05-29 02:18:02,189: INFO: model_trainer: textClassifierLogger: Test results: {'test_acc': 0.125, 'test_precision': 0.12406015396118164, 'test_recall': 0.125, 'test_f1': 0.12445278465747833}]\n"
     ]
    }
   ],
   "source": [
    "test_results, model_path = model_trainer.train_model('bert')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "db07ee68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'test_acc': 0.125,\n",
       "  'test_precision': 0.12406015396118164,\n",
       "  'test_recall': 0.125,\n",
       "  'test_f1': 0.12445278465747833},\n",
       " 'artifacts/model_trainer/bert/model.ckpt')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_results, model_path "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d544eb7c",
   "metadata": {},
   "source": [
    "### roberta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e053d0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "roberta = model_trainer.get_model('roberta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c7eccdc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable: classifier.weight\n",
      "Trainable: classifier.bias\n"
     ]
    }
   ],
   "source": [
    "for name, param in roberta.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f\"Trainable: {name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "03d12d78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1538"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.numel() for p in roberta.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0d83feb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-29 02:36:45,129: INFO: model_trainer: textClassifierLogger: Starting training for roberta]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/05/29 02:36:45 INFO mlflow.tracking.fluent: Experiment with name 'text_classification_roberta' does not exist. Creating a new experiment.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-29 02:36:47,570: INFO: setup: pytorch_lightning.utilities.rank_zero: GPU available: False, used: False]\n",
      "[2025-05-29 02:36:47,577: INFO: setup: pytorch_lightning.utilities.rank_zero: TPU available: False, using: 0 TPU cores]\n",
      "[2025-05-29 02:36:47,583: INFO: setup: pytorch_lightning.utilities.rank_zero: HPU available: False, using: 0 HPUs]\n",
      "[2025-05-29 02:36:47,813: INFO: model_summary: pytorch_lightning.callbacks.model_summary: \n",
      "  | Name       | Type             | Params | Mode \n",
      "--------------------------------------------------------\n",
      "0 | roberta    | RobertaModel     | 124 M  | eval \n",
      "1 | dropout    | Dropout          | 0      | train\n",
      "2 | classifier | Linear           | 1.5 K  | train\n",
      "3 | criterion  | CrossEntropyLoss | 0      | train\n",
      "--------------------------------------------------------\n",
      "1.5 K     Trainable params\n",
      "124 M     Non-trainable params\n",
      "124 M     Total params\n",
      "498.589   Total estimated model params size (MB)\n",
      "3         Modules in train mode\n",
      "228       Modules in eval mode]\n",
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mohamed-stifi/Desktop/pfa-s4/pfa-venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mohamed-stifi/Desktop/pfa-s4/pfa-venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  28%|██▊       | 5/18 [01:30<03:55,  0.06it/s, v_num=5bea, train_loss=0.717, train_acc=0.250, val_loss=0.700, val_acc=0.500][2025-05-29 02:38:58,556: INFO: model_checkpoint: pytorch_lightning.utilities.rank_zero: Epoch 0, global step 5: 'val_acc' reached 0.50000 (best 0.50000), saving model to '/home/mohamed-stifi/Desktop/pfa-s4/artifacts/model_trainer/roberta/roberta-epoch=00-val_acc=0.50.ckpt' as top 1]\n",
      "Epoch 0:  56%|█████▌    | 10/18 [02:54<02:19,  0.06it/s, v_num=5bea, train_loss=0.699, train_acc=0.500, val_loss=0.699, val_acc=0.500][2025-05-29 02:40:22,177: INFO: model_checkpoint: pytorch_lightning.utilities.rank_zero: Epoch 0, global step 10: 'val_acc' was not in top 1]\n",
      "Epoch 0:  83%|████████▎ | 15/18 [04:07<00:49,  0.06it/s, v_num=5bea, train_loss=0.696, train_acc=0.500, val_loss=0.699, val_acc=0.500][2025-05-29 02:41:35,863: INFO: model_checkpoint: pytorch_lightning.utilities.rank_zero: Epoch 0, global step 15: 'val_acc' was not in top 1]\n",
      "Epoch 0: 100%|██████████| 18/18 [04:39<00:00,  0.06it/s, v_num=5bea, train_loss=0.687, train_acc=0.500, val_loss=0.699, val_acc=0.500][2025-05-29 02:42:07,837: INFO: fit_loop: pytorch_lightning.utilities.rank_zero: `Trainer.fit` stopped: `max_epochs=1` reached.]\n",
      "Epoch 0: 100%|██████████| 18/18 [04:39<00:00,  0.06it/s, v_num=5bea, train_loss=0.687, train_acc=0.500, val_loss=0.699, val_acc=0.500]\n",
      "[2025-05-29 02:42:07,891: INFO: checkpoint_connector: pytorch_lightning.utilities.rank_zero: Restoring states from the checkpoint path at /home/mohamed-stifi/Desktop/pfa-s4/artifacts/model_trainer/roberta/roberta-epoch=00-val_acc=0.50.ckpt]\n",
      "[2025-05-29 02:42:11,378: INFO: checkpoint_connector: pytorch_lightning.utilities.rank_zero: Loaded model weights from the checkpoint at /home/mohamed-stifi/Desktop/pfa-s4/artifacts/model_trainer/roberta/roberta-epoch=00-val_acc=0.50.ckpt]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mohamed-stifi/Desktop/pfa-s4/pfa-venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 5/5 [01:24<00:00,  0.06it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mohamed-stifi/Desktop/pfa-s4/pfa-venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        test_acc                    0.5\n",
      "         test_f1            0.3333333432674408\n",
      "     test_precision                0.25\n",
      "       test_recall                  0.5\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/05/29 02:44:57 WARNING mlflow.utils.requirements_utils: The following packages were not found in the public PyPI package index as of 2025-04-15; if these packages are not present in the public PyPI index, you must install them manually before loading your model: {'text-classifier'}\n",
      "\u001b[31m2025/05/29 02:44:58 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-29 02:45:15,016: INFO: model_trainer: textClassifierLogger: Training completed for roberta]\n",
      "[2025-05-29 02:45:15,046: INFO: model_trainer: textClassifierLogger: Test results: {'test_acc': 0.5, 'test_precision': 0.25, 'test_recall': 0.5, 'test_f1': 0.3333333432674408}]\n"
     ]
    }
   ],
   "source": [
    "test_results, model_path = model_trainer.train_model('roberta')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0cecc1",
   "metadata": {},
   "source": [
    "## train all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fb0e73a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on ModelTrainer in module text_classifier.components.model_trainer object:\n",
      "\n",
      "class ModelTrainer(builtins.object)\n",
      " |  ModelTrainer(config: text_classifier.entity.config_entity.ModelTrainerConfig)\n",
      " |\n",
      " |  Methods defined here:\n",
      " |\n",
      " |  __init__(self, config: text_classifier.entity.config_entity.ModelTrainerConfig)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |\n",
      " |  get_model(self, model_name, vocab_size=None)\n",
      " |      Get model based on model name\n",
      " |\n",
      " |  prepare_data_loaders(self, model_name)\n",
      " |      Prepare data loaders for training\n",
      " |\n",
      " |  train_all_models(self)\n",
      " |      Train all models and compare results\n",
      " |\n",
      " |  train_model(self, model_name)\n",
      " |      Train a specific model\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |\n",
      " |  __dict__\n",
      " |      dictionary for instance variables\n",
      " |\n",
      " |  __weakref__\n",
      " |      list of weak references to the object\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(model_trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60a90ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-29 16:07:07,809: INFO: model_trainer: textClassifierLogger: Starting training for lstm]\n",
      "[2025-05-29 16:07:11,386: INFO: setup: pytorch_lightning.utilities.rank_zero: GPU available: False, used: False]\n",
      "[2025-05-29 16:07:11,389: INFO: setup: pytorch_lightning.utilities.rank_zero: TPU available: False, using: 0 TPU cores]\n",
      "[2025-05-29 16:07:11,394: INFO: setup: pytorch_lightning.utilities.rank_zero: HPU available: False, using: 0 HPUs]\n",
      "[2025-05-29 16:07:11,560: INFO: model_summary: pytorch_lightning.callbacks.model_summary: \n",
      "  | Name       | Type             | Params | Mode \n",
      "--------------------------------------------------------\n",
      "0 | embedding  | Embedding        | 409 K  | train\n",
      "1 | lstm       | LSTM             | 2.4 M  | train\n",
      "2 | dropout    | Dropout          | 0      | train\n",
      "3 | classifier | Linear           | 1.0 K  | train\n",
      "4 | criterion  | CrossEntropyLoss | 0      | train\n",
      "--------------------------------------------------------\n",
      "2.8 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.8 M     Total params\n",
      "11.111    Total estimated model params size (MB)\n",
      "5         Modules in train mode\n",
      "0         Modules in eval mode]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mohamed-stifi/Desktop/pfa-s4/pfa-venv/lib/python3.12/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/mohamed-stifi/Desktop/pfa-s4/artifacts/model_trainer/lstm exists and is not empty.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mohamed-stifi/Desktop/pfa-s4/pfa-venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mohamed-stifi/Desktop/pfa-s4/pfa-venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  28%|██▊       | 5/18 [02:36<06:47,  0.03it/s, v_num=f0d6, train_loss=0.686, train_acc=0.750, val_loss=0.690, val_acc=0.550][2025-05-29 16:09:52,699: INFO: model_checkpoint: pytorch_lightning.utilities.rank_zero: Epoch 0, global step 5: 'val_acc' reached 0.55000 (best 0.55000), saving model to '/home/mohamed-stifi/Desktop/pfa-s4/artifacts/model_trainer/lstm/lstm-epoch=00-val_acc=0.55.ckpt' as top 1]\n",
      "Epoch 0:  56%|█████▌    | 10/18 [05:23<04:18,  0.03it/s, v_num=f0d6, train_loss=0.689, train_acc=0.875, val_loss=0.689, val_acc=0.700][2025-05-29 16:12:39,125: INFO: model_checkpoint: pytorch_lightning.utilities.rank_zero: Epoch 0, global step 10: 'val_acc' reached 0.70000 (best 0.70000), saving model to '/home/mohamed-stifi/Desktop/pfa-s4/artifacts/model_trainer/lstm/lstm-epoch=00-val_acc=0.70.ckpt' as top 1]\n",
      "Epoch 0:  83%|████████▎ | 15/18 [07:57<01:35,  0.03it/s, v_num=f0d6, train_loss=0.695, train_acc=0.250, val_loss=0.687, val_acc=0.650][2025-05-29 16:15:13,131: INFO: model_checkpoint: pytorch_lightning.utilities.rank_zero: Epoch 0, global step 15: 'val_acc' was not in top 1]\n",
      "Epoch 0: 100%|██████████| 18/18 [09:05<00:00,  0.03it/s, v_num=f0d6, train_loss=0.692, train_acc=0.250, val_loss=0.687, val_acc=0.650][2025-05-29 16:16:21,537: INFO: fit_loop: pytorch_lightning.utilities.rank_zero: `Trainer.fit` stopped: `max_epochs=1` reached.]\n",
      "Epoch 0: 100%|██████████| 18/18 [09:05<00:00,  0.03it/s, v_num=f0d6, train_loss=0.692, train_acc=0.250, val_loss=0.687, val_acc=0.650]\n",
      "[2025-05-29 16:16:21,986: INFO: checkpoint_connector: pytorch_lightning.utilities.rank_zero: Restoring states from the checkpoint path at /home/mohamed-stifi/Desktop/pfa-s4/artifacts/model_trainer/lstm/lstm-epoch=00-val_acc=0.70.ckpt]\n",
      "[2025-05-29 16:16:22,213: INFO: checkpoint_connector: pytorch_lightning.utilities.rank_zero: Loaded model weights from the checkpoint at /home/mohamed-stifi/Desktop/pfa-s4/artifacts/model_trainer/lstm/lstm-epoch=00-val_acc=0.70.ckpt]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mohamed-stifi/Desktop/pfa-s4/pfa-venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 5/5 [00:04<00:00,  1.16it/s]\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        test_acc             0.824999988079071\n",
      "         test_f1            0.8240100741386414\n",
      "     test_precision         0.8324808478355408\n",
      "       test_recall           0.824999988079071\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/05/29 16:18:31 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /tmp/tmpt13itje7/model/data, flavor: pytorch). Fall back to return ['torch==2.7.0', 'cloudpickle==3.1.1']. Set logging level to DEBUG to see the full traceback. \n",
      "\u001b[31m2025/05/29 16:18:32 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-29 16:18:32,917: INFO: model_trainer: textClassifierLogger: Training completed for lstm]\n",
      "[2025-05-29 16:18:32,941: INFO: model_trainer: textClassifierLogger: Test results: {'test_acc': 0.824999988079071, 'test_precision': 0.8324808478355408, 'test_recall': 0.824999988079071, 'test_f1': 0.8240100741386414}]\n",
      "[2025-05-29 16:18:33,110: INFO: model_trainer: textClassifierLogger: Starting training for bert]\n",
      "[2025-05-29 16:18:45,600: INFO: setup: pytorch_lightning.utilities.rank_zero: GPU available: False, used: False]\n",
      "[2025-05-29 16:18:45,626: INFO: setup: pytorch_lightning.utilities.rank_zero: TPU available: False, using: 0 TPU cores]\n",
      "[2025-05-29 16:18:45,628: INFO: setup: pytorch_lightning.utilities.rank_zero: HPU available: False, using: 0 HPUs]\n",
      "[2025-05-29 16:18:45,961: INFO: model_summary: pytorch_lightning.callbacks.model_summary: \n",
      "  | Name       | Type             | Params | Mode \n",
      "--------------------------------------------------------\n",
      "0 | bert       | BertModel        | 109 M  | eval \n",
      "1 | dropout    | Dropout          | 0      | train\n",
      "2 | classifier | Linear           | 1.5 K  | train\n",
      "3 | criterion  | CrossEntropyLoss | 0      | train\n",
      "--------------------------------------------------------\n",
      "1.5 K     Trainable params\n",
      "109 M     Non-trainable params\n",
      "109 M     Total params\n",
      "437.935   Total estimated model params size (MB)\n",
      "3         Modules in train mode\n",
      "228       Modules in eval mode]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mohamed-stifi/Desktop/pfa-s4/pfa-venv/lib/python3.12/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/mohamed-stifi/Desktop/pfa-s4/artifacts/model_trainer/bert exists and is not empty.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mohamed-stifi/Desktop/pfa-s4/pfa-venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mohamed-stifi/Desktop/pfa-s4/pfa-venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  28%|██▊       | 5/18 [02:24<06:16,  0.03it/s, v_num=8ee4, train_loss=0.701, train_acc=0.375, val_loss=0.660, val_acc=0.500][2025-05-29 16:21:46,978: INFO: model_checkpoint: pytorch_lightning.utilities.rank_zero: Epoch 0, global step 5: 'val_acc' reached 0.50000 (best 0.50000), saving model to '/home/mohamed-stifi/Desktop/pfa-s4/artifacts/model_trainer/bert/bert-epoch=00-val_acc=0.50-v2.ckpt' as top 1]\n",
      "Epoch 0:  56%|█████▌    | 10/18 [05:09<04:07,  0.03it/s, v_num=8ee4, train_loss=0.676, train_acc=0.625, val_loss=0.658, val_acc=0.500][2025-05-29 16:24:31,621: INFO: model_checkpoint: pytorch_lightning.utilities.rank_zero: Epoch 0, global step 10: 'val_acc' was not in top 1]\n",
      "Epoch 0:  83%|████████▎ | 15/18 [07:24<01:28,  0.03it/s, v_num=8ee4, train_loss=0.601, train_acc=0.625, val_loss=0.657, val_acc=0.500][2025-05-29 16:26:46,911: INFO: model_checkpoint: pytorch_lightning.utilities.rank_zero: Epoch 0, global step 15: 'val_acc' was not in top 1]\n",
      "Epoch 0: 100%|██████████| 18/18 [08:11<00:00,  0.04it/s, v_num=8ee4, train_loss=0.591, train_acc=0.750, val_loss=0.657, val_acc=0.500][2025-05-29 16:27:33,525: INFO: fit_loop: pytorch_lightning.utilities.rank_zero: `Trainer.fit` stopped: `max_epochs=1` reached.]\n",
      "Epoch 0: 100%|██████████| 18/18 [08:11<00:00,  0.04it/s, v_num=8ee4, train_loss=0.591, train_acc=0.750, val_loss=0.657, val_acc=0.500]\n",
      "[2025-05-29 16:27:33,705: INFO: checkpoint_connector: pytorch_lightning.utilities.rank_zero: Restoring states from the checkpoint path at /home/mohamed-stifi/Desktop/pfa-s4/artifacts/model_trainer/bert/bert-epoch=00-val_acc=0.50-v2.ckpt]\n"
     ]
    }
   ],
   "source": [
    "comparison_results = model_trainer.train_all_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3997c635",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pfa-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
